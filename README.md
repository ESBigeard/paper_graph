

# Paper Graph
Dev/tools repo for a project about scientific papers mining to construct graphs

[bibliography has been moved to its own file](REFERENCES.md)

If you want to execute the code you can find the corpus [here](https://www.dropbox.com/s/0bc6c2fmhz526mo/fulltext_tei.tar.gz?dl=0]). It's TEI XML files.

## Working pipeline

Various steps need an idf inside a .pickle file. I've added one to the repository for convenience, but you can generate it with utilsperso.py The idf must be trained on a corpus identical or similar than the corpus we're working on.

MAIN : PDF of articles collected by canceropole -> grobit -> TEI XML files -> [generate_gephi_csv.py](generate_gephi_csv.py) -> nodes.csv and edges.csv -> aman's script to add similar nodes and coordinates -> [convert_id_to_tile.py](convert_id_to_title) (aman gives similar nodes by id. this converts the id to the label) -> nodes.csv and edges.csv -> gephi -> GEXF XML file -> [this javascript website with few changes](https://github.com/raphv/gexf-js)

For the html files displaying the content of the articles, input the TEI XML files into [generate_html_article_pages.py](generate_html_article_pages.py)


## Main scripts and useful stuff

[takes a folder of tei xml generated by grobit, outputs  nodes.csv and edges.csv ready for gephi](generate_gephi_csv.py)

[necessary to make anything else run](utilsperso.py)

[creates the html pages for each article with the main sentences highlighted in yellow](generate_html_article_pages.py)

[for the most similar nodes added by aman, replace the ID of each node by its label](convert_id_to_title.py)
