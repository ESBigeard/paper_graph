#!/usr/bin/python
# -*- coding:utf-8 -*-
"""finds relevant section titles. This module is used by several other scripts, be prudent making changes"""

import re
import spacy
import pickle
import os
import sys
from collections import defaultdict,Counter
from bs4 import BeautifulSoup


nlp=spacy.load("en_core_web_sm")
sentence_tokeniser=nlp.create_pipe("sentencizer")
nlp.add_pipe(sentence_tokeniser)

def finish_idf(idf):
	"""do that to an idf generated by edit_idf (stored in a idf.pickle). edit_idf only generates a dictionnary of word occurences. this turns it into word frequencies"""

	max_freq=float(idf.most_common(1)[0][1])
	for word in idf:
		idf[word]=idf[word]/max_freq
	return idf

def _get_words_from_grobit_xml(filename):
	"""from a grobit xml file, extracts the list of all encountered words. returns a list of words in the order they were encountered, so they typically appear several times. stopwords etc are removed"""
	wordlist=[]

	if type(filename)==str:
		with open(filename,mode="r",encoding="utf-8") as f:
			lines="".join(f.readlines())
			soup=BeautifulSoup(lines,"xml")
	elif type(filename)==bs4.BeautifulSoup:
		soup=filename
	else:
		raise TypeError("argument filename should be a path to a file or a beautifulsoup")

	for tags in [soup("ref")]:
		for tag in tags:
			tag.decompose()

	#identify the sections of the text
	#we care about the sections to be able to exclude stuff like "acknowledgment"
	current_main_title="other"
	for tag in soup.find_all(["head","p"]):
		if tag.name=="head":
			title_clean=process_section_title(tag.getText()) 
			if title_clean:
				if title_clean == "other": #random title in the body
					pass
				else: #a maintitle such as "introduction"
					current_main_title=title_clean
			else: #title of something we don't want to keep
				current_main_title="trash"

		else: #p tag, we want to get the text
			sentences=preprocess_text(tag,keep_all_words=False)
			if current_main_title=="trash":
				continue
			else:
				#we have clean text, we add it to the wordlist
				for sent in sentences:
					for token in sent:
						if token.pos_ == "NUM":
							wordlist.append("NUM")
						wordlist.append(token.lemma_)

	return wordlist


def edit_idf(documents_list,filetype="",idf_file="idf.pickle"):
	"""add the frequencies of words from new documents to an idf count. if the idf_file specified doesn't exist, a new idf count is initiated.
	Warning : Before using the generated idf, remember to divide each entry by the max frequency ! Raw occurences are stored, not frequencies !
	argument documents_list : list of paths of files to process
	argument filetype : xml generated by grobit ? pubmed full text xml ?
	argument idf_file : a pickle file containing a previously saved idf. if the files doesn't exist a new idf will be started and stored in this file
	assumes that along the idf_file "filename.pickle" stands a file "filename.files.pickle" that stores the list of paths of already processed files. This allows to interrupt and resume the processing easily. Files in this pickle will not be processed.
	"""

	#check input integrity
	if not filetype in ["grobit_xml"]: #list of types currently processed
		raise ValueError("Unknown value for argument filetype")

	if not idf_file.endswith(".pickle"):
		raise ValueError("Argument idf_file must be a pickle file")

	#load idf
	start_over=False
	try:
		with open(idf_file,mode="rb") as f:
			idf=pickle.load(f)
			
	#if we can't load the idf, we start a new one
	except FileNotFoundError:
		sys.stderr.write("Starting a new idf in file "+idf_file+"\n")
		start_over=True
		idf=Counter()
		processed_files_idf=[]
	
	idf_file2=idf_file.split(".")[0]+".files.pickle"
	if not start_over:
		try:
			with open(idf_file2,mode="rb") as f:
				processed_files_idf=pickle.load(f)
		except FileNotFoundError:
			raise FileNotFoundError("There should be a file named "+idf_file2+" next to the idf file. If this file is missing it's better to start over with a new idf. Remove your current idf file or specify a new path.")
	

	#start processing the files
	try:
		for i,fname in enumerate(documents_list):

			sys.stderr.write("Calculating idf of "+fname+"\n")
			
			if filetype=="grobit_xml":

					wordlist=_get_words_from_grobit_xml(fname)
					for word in wordlist:
						idf[word]+=1
			else:
				sys.stderr.write("Other formats to come later...\n")

			processed_files_idf.append(fname)
		
			if i%20==0:
				sys.stderr.write("Reminder : You can KeyboardInterrupt at any time. This will save the current state of the IDF to disk.\n")
	
	except KeyboardInterrupt: #this breaks the iteration over entry files. Allows the user to stop processing new files and proceed with the rest of the script
		pass

	#do that before using the idf
	#max_freq=float(idf.most_common(1)[0][1])
	#for word in idf:
	#	idf[word]=idf[word]/max_freq

	with open(idf_file,mode="wb") as f:
		pickle.dump(idf,f)
	with open(idf_file2,mode="wb") as f:
		pickle.dump(processed_files_idf,f)
	sys.stderr.write("IDF saved !\n")

	return

def count_text_tfidf(text,idf=False):
	"""text argument : either a text or a list of words from a text already tokenised. words should be already preprocessed : lemmatised, stop-words filtered...
	idf argument : a word frequency dictionnary. If not specified, will look for a idf.pickle file in the working directory
	output : tf/idf dictionnary"""

	if not idf:
		with open("idf.pickle",mode="rb") as f:
			idf=pickle.load(f)

	tf=defaultdict(int)
	if type(text)==str:
		text=re.split("\W+",text)
	
	for word in text:
		word=word.lower() #preprocessing should have been done outside of this function, but can't hurt to do something very basic just in case. just in case of uppercase. haha. sorry.
		tf[word]+=1
	
	for word in tf:
		if word in idf:
			tf[word]=tf[word]/idf[word]
		else:
			tf[word]=0 #may happen if the text we're processing wasn't in the corpus to build the idf

	return tf

def _clean_section_title(string):
	
	if len(string)<1:
		return string
	string=string.lower()
	if string[-1]=="s" :#or string[-1]==".":
		#s = fix plural variability
		string=string[:-1]
		
	#string=string.replace("â€™","'") 
	string=re.sub("[^ \w]","",string)
	string=re.sub("\d","",string)
	string=re.sub("&","and",string)
	string=string.strip()

	return string

def process_section_title(raw_title):
	"""argument : raw section title
	returns the clean title. bad titles such as acknowledgment will return False. uncommon titles will return 'other' returns None if the title is empty"""

	good_titles=["introduction","method","conclusion","finding","result","discussion","background","materials and method","material and method"]
	bad_titles=["acknowledgement","acknowledgment","source of funding","competing interest","author contribution","authors contribution","supplementary information","figure","fig","table","conflict of interest","conflicts of interest"]
	title_clean=_clean_section_title(raw_title)
	if len(title_clean)>1:
		if title_clean in good_titles :
			return title_clean
		elif title_clean in bad_titles:
			return False
		else:
			return "other"


def preprocess_text(soup,keep_all_words=False):
	""" argument soup : beautifulsoup containing text. can be a section of soup. extract and processes all text inside that soup.
	returns list of spacy sentences with only words that were kept for the idf.
	if argument keep_all_words=True : returns list of spacy sentences with all words left, including stopwords
	warning : when calculating the idf I usually switch numbers for NUM. Here the numbers are returned as untounched tokens. Numbers should be detected by token.pos_=="NUM" """
	
	text=soup.getText(separator=" ")
	text=re.sub("\. ?\. ",". ",text)
	text=re.sub("et al \.","et al ",text)
	text=re.sub(" +"," ",text)
		

	spacy_text=nlp(text)
	sentences=[]
	for sentence in spacy_text.sents:
		
		current_sent=[]
		for token in sentence:

			if keep_all_words:
				current_sent.append(token)

			else:
				if not token.is_stop:

					if token.pos_ == "NUM":
						current_sent.append(token)
						#warning, when calculating the idf we switched the lemma for "NUM"
						#current_sent.append("NUM")
					elif  token.pos_ in ["NOUN""VERB","PROPN","ADJ"]:
						current_sent.append(token)
		sentences.append(current_sent)
	return sentences


if __name__=="__main__":
	pass

	#generate idf for a folder
	#directory="/media/ezi/Backup1/dl_nancy/Canceropole_dropbox/fulltext3_tei"
	#files=[directory+"/"+x for x in os.listdir(directory)]
	#edit_idf(files,filetype="grobit_xml",idf_file="dropbox_tei.pickle")
